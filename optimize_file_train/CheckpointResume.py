# CheckpointResume.py
# ‚úÖ Code ƒë·ªÉ resume training t·ª´ checkpoint n·∫øu Colab disconnect
# Th√™m v√†o cell training (Section 8), TR∆Ø·ªöC trainer.train()

import os
import glob


def find_latest_checkpoint(output_dir):
    """
    T√¨m checkpoint m·ªõi nh·∫•t trong output directory
    
    Args:
        output_dir: Directory ch·ª©a checkpoints
    
    Returns:
        str or None: Path to latest checkpoint, or None if not found
    """
    if not os.path.exists(output_dir):
        return None
    
    # T√¨m t·∫•t c·∫£ checkpoints (format: checkpoint-{step})
    checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    
    if not checkpoints:
        return None
    
    # Extract step numbers
    checkpoint_steps = []
    for cp in checkpoints:
        try:
            step = int(os.path.basename(cp).split('-')[1])
            checkpoint_steps.append((step, cp))
        except (IndexError, ValueError):
            continue
    
    if not checkpoint_steps:
        return None
    
    # Sort by step v√† l·∫•y checkpoint m·ªõi nh·∫•t
    checkpoint_steps.sort(key=lambda x: x[0], reverse=True)
    latest_step, latest_checkpoint = checkpoint_steps[0]
    
    return latest_checkpoint


def get_checkpoint_info(checkpoint_dir):
    """
    L·∫•y th√¥ng tin v·ªÅ checkpoint
    
    Returns:
        dict: Checkpoint information
    """
    if not checkpoint_dir or not os.path.exists(checkpoint_dir):
        return None
    
    # Parse step from dirname
    try:
        step = int(os.path.basename(checkpoint_dir).split('-')[1])
    except (IndexError, ValueError):
        step = None
    
    # Check files
    has_model = os.path.exists(os.path.join(checkpoint_dir, "pytorch_model.bin"))
    has_config = os.path.exists(os.path.join(checkpoint_dir, "config.json"))
    has_optimizer = os.path.exists(os.path.join(checkpoint_dir, "optimizer.pt"))
    has_scheduler = os.path.exists(os.path.join(checkpoint_dir, "scheduler.pt"))
    
    return {
        'path': checkpoint_dir,
        'step': step,
        'has_model': has_model,
        'has_config': has_config,
        'has_optimizer': has_optimizer,
        'has_scheduler': has_scheduler,
        'complete': all([has_model, has_config, has_optimizer, has_scheduler])
    }


def clean_incomplete_checkpoints(output_dir):
    """
    X√≥a checkpoints kh√¥ng ho√†n ch·ªânh (b·ªã corrupt khi disconnect)
    """
    checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    
    cleaned = 0
    for cp in checkpoints:
        info = get_checkpoint_info(cp)
        if info and not info['complete']:
            print(f"üóëÔ∏è Removing incomplete checkpoint: {os.path.basename(cp)}")
            import shutil
            shutil.rmtree(cp)
            cleaned += 1
    
    if cleaned > 0:
        print(f"‚úÖ Cleaned {cleaned} incomplete checkpoint(s)")
    
    return cleaned


def setup_checkpoint_resume(config):
    """
    Setup checkpoint resume v·ªõi validation
    
    Returns:
        str or None: Checkpoint path to resume from
    """
    print("\nüîÑ Checking for existing checkpoints...")
    print("="*60)
    
    # Clean incomplete checkpoints first
    clean_incomplete_checkpoints(config.output_dir)
    
    # Find latest
    checkpoint_dir = find_latest_checkpoint(config.output_dir)
    
    if checkpoint_dir:
        info = get_checkpoint_info(checkpoint_dir)
        
        if info and info['complete']:
            print(f"‚úÖ Found complete checkpoint:")
            print(f"   Path: {checkpoint_dir}")
            print(f"   Step: {info['step']:,}")
            print(f"   Files: Model ‚úì | Config ‚úì | Optimizer ‚úì | Scheduler ‚úì")
            print()
            
            # Estimate progress
            if info['step']:
                # Rough estimation (depends on total steps)
                print(f"üìä Training will resume from step {info['step']:,}")
            
            print("="*60)
            return checkpoint_dir
        else:
            print("‚ö†Ô∏è Found checkpoint but it's incomplete. Starting fresh.")
            print("="*60)
            return None
    else:
        print("‚ÑπÔ∏è No checkpoint found. Starting from scratch.")
        print("="*60)
        return None


# ============================================================================
# USAGE IN NOTEBOOK - TRAINING CELL
# ============================================================================
"""
# REPLACE your training cell with this:

# Setup checkpoint resume
checkpoint_path = setup_checkpoint_resume(config)

# Training arguments (same as before)
training_args = TrainingArguments(
    output_dir=config.output_dir,
    num_train_epochs=config.num_epochs,
    per_device_train_batch_size=config.batch_size,
    per_device_eval_batch_size=config.batch_size * 2,
    learning_rate=config.learning_rate,
    warmup_steps=config.warmup_steps,
    weight_decay=config.weight_decay,
    
    # Logging & Checkpointing
    logging_dir=f"{config.output_dir}/logs",
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=3,  # Keep only 3 latest checkpoints
    
    # Metrics
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",  # ‚úÖ Changed from f1_weighted
    greater_is_better=True,
    
    # Performance
    gradient_accumulation_steps=4,  # ‚úÖ Added
    fp16=torch.cuda.is_available(),
    
    report_to="none",
)

# Setup trainer (with class weights if available)
if 'class_weights' in globals():
    trainer = WeightedLossTrainer(
        class_weights=class_weights,
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )
else:
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

print("‚úÖ Trainer ready")

# Train with resume
print("\nüöÄ Starting training...")
print("="*70)

if checkpoint_path:
    print(f"‚ñ∂Ô∏è RESUMING from: {checkpoint_path}\n")
    trainer.train(resume_from_checkpoint=checkpoint_path)
else:
    print("‚ñ∂Ô∏è STARTING fresh training\n")
    trainer.train()

print("\n‚úÖ Training complete!")
"""


# ============================================================================
# ADVANCED: Auto-save on disconnect detection
# ============================================================================

class DisconnectSafeTrainer(Trainer):
    """
    Trainer that saves checkpoint more frequently for Colab
    """
    
    def __init__(self, *args, save_every_n_steps=100, **kwargs):
        super().__init__(*args, **kwargs)
        self.save_every_n_steps = save_every_n_steps
        self._last_save_step = 0
    
    def training_step(self, *args, **kwargs):
        """Override to add frequent checkpointing"""
        loss = super().training_step(*args, **kwargs)
        
        # Save every N steps
        if self.state.global_step - self._last_save_step >= self.save_every_n_steps:
            self.save_model()
            self._last_save_step = self.state.global_step
            print(f"üíæ Auto-saved checkpoint at step {self.state.global_step}")
        
        return loss


# Usage:
"""
trainer = DisconnectSafeTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    save_every_n_steps=100,  # Save every 100 steps
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
"""


# ============================================================================
# MONITORING
# ============================================================================

def print_gpu_usage():
    """Print current GPU usage (Colab specific)"""
    try:
        import subprocess
        result = subprocess.check_output(
            ['nvidia-smi', '--query-gpu=memory.used,memory.total,utilization.gpu',
             '--format=csv,noheader,nounits'],
            encoding='utf-8'
        )
        
        memory_used, memory_total, gpu_util = result.strip().split(',')
        
        print(f"\nüñ•Ô∏è GPU Status:")
        print(f"  Memory: {memory_used.strip()}MB / {memory_total.strip()}MB")
        print(f"  Utilization: {gpu_util.strip()}%")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Could not get GPU info: {e}")


def estimate_remaining_time(trainer):
    """Estimate remaining training time"""
    if not hasattr(trainer, 'state') or not trainer.state:
        return None
    
    current_step = trainer.state.global_step
    max_steps = trainer.state.max_steps
    
    if current_step == 0 or max_steps == 0:
        return None
    
    # Estimate based on current progress
    progress = current_step / max_steps
    # This is rough, actual time varies
    
    return {
        'current_step': current_step,
        'max_steps': max_steps,
        'progress': progress * 100
    }


# Usage:
"""
# After training starts, in a separate cell:
info = estimate_remaining_time(trainer)
if info:
    print(f"Progress: {info['progress']:.1f}% ({info['current_step']}/{info['max_steps']})")

print_gpu_usage()
"""
