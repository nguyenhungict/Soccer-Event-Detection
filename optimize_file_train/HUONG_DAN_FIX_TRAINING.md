# ğŸ”§ HÆ¯á»šNG DáºªN FIX CODE TRAINING - RETRAIN MODEL

## ğŸ“‹ TÃ“M Táº®T PHÃ‚N TÃCH

TÃ´i Ä‘Ã£ Ä‘á»c Ä‘Æ°á»£c toÃ n bá»™ notebook cá»§a báº¡n. PhÃ¡t hiá»‡n **Váº¤N Äá»€ NGHIÃŠM TRá»ŒNG**:

### âŒ Váº¥n Ä‘á» hiá»‡n táº¡i:
```
TRÆ¯á»šC balance:  No-Event: 1,058,893 (98.7%)
SAU balance:    No-Event: 158,833 (91.8%)  â† VáºªN QUÃ CAO!
```

**Root cause:** Config `no_event_keep_ratio = 0.15` nghÄ©a lÃ  "giá»¯ 15% cá»§a No-Event", KHÃ”NG pháº£i "No-Event chiáº¿m 15% tá»•ng dataset"!

### âœ… Káº¿t quáº£ mong muá»‘n:
```
No-Event: ~40-50% cá»§a tá»•ng dataset (thay vÃ¬ 91.8%)
Events:   ~50-60%
```

---

## ğŸ¯ CÃC PHáº¦N Cáº¦N Sá»¬A

### **Fix 1: Config - DÃ²ng 355-357 (QUAN TRá»ŒNG NHáº¤T)**

**Code CÅ¨ (SAI):**
```python
# TÄƒng tá»« 3% lÃªn 15%. Model sáº½ cÃ³ nhiá»u dá»¯ liá»‡u ná»n Ä‘á»ƒ so sÃ¡nh hÆ¡n.
no_event_keep_ratio: float = 0.15
```

**Code Má»šI (ÄÃšNG):**
```python
# Target: No-Event chiáº¿m 40-50% tá»•ng dataset
# ChÃºng ta sáº½ tÃ­nh Ä‘á»™ng dá»±a trÃªn sá»‘ lÆ°á»£ng events
no_event_target_ratio: float = 0.5  # No-Event = 50% dataset
# Hoáº·c dÃ¹ng multiplier: No-Event = 2x sá»‘ events
no_event_multiplier: int = 2  # No-Event samples = 2 * Event samples
```

---

### **Fix 2: ClassBalancer - DÃ²ng 687-715 (CORE LOGIC)**

**Code CÅ¨ (SAI):**
```python
class ClassBalancer:
    def __init__(self, config: Config):
        self.config = config

    def balance_dataset(self, windows: List[Dict]) -> List[Dict]:
        event_windows = []
        no_event_windows = []

        for window in windows:
            if window['label'] == 'No-Event':
                no_event_windows.append(window)
            else:
                event_windows.append(window)

        # âŒ SAI: Giá»¯ 15% cá»§a No-Event gá»‘c
        num_to_keep = int(len(no_event_windows) * self.config.no_event_keep_ratio)
        kept_no_event = random.sample(no_event_windows, num_to_keep)

        balanced = event_windows + kept_no_event
        random.shuffle(balanced)
        return balanced
```

**Code Má»šI (ÄÃšNG) - OPTION 1 (Recommended):**
```python
class ClassBalancer:
    def __init__(self, config: Config):
        self.config = config

    def balance_dataset(self, windows: List[Dict]) -> List[Dict]:
        event_windows = []
        no_event_windows = []

        for window in windows:
            if window['label'] == 'No-Event':
                no_event_windows.append(window)
            else:
                event_windows.append(window)

        # âœ… ÄÃšNG: Downsample No-Event dá»±a trÃªn sá»‘ event
        num_events = len(event_windows)
        
        # CÃ¡ch 1: DÃ¹ng multiplier (No-Event = 2x events)
        if hasattr(self.config, 'no_event_multiplier'):
            target_no_event = num_events * self.config.no_event_multiplier
        # CÃ¡ch 2: DÃ¹ng ratio (No-Event = 50% tá»•ng)
        elif hasattr(self.config, 'no_event_target_ratio'):
            ratio = self.config.no_event_target_ratio
            # no_event / (events + no_event) = ratio
            # no_event = ratio * (events + no_event)
            # no_event = ratio * events / (1 - ratio)
            target_no_event = int(num_events * ratio / (1 - ratio))
        else:
            # Fallback: 2x events
            target_no_event = num_events * 2

        # Giá»›i háº¡n khÃ´ng vÆ°á»£t quÃ¡ sá»‘ No-Event cÃ³ sáºµn
        num_to_keep = min(target_no_event, len(no_event_windows))
        kept_no_event = random.sample(no_event_windows, num_to_keep)

        balanced = event_windows + kept_no_event
        random.shuffle(balanced)

        # Print statistics
        total = len(balanced)
        print(f"\nğŸ¯ Class Balancing:")
        print(f"  Original No-Event: {len(no_event_windows):,}")
        print(f"  Event samples: {num_events:,}")
        print(f"  Target No-Event: {target_no_event:,}")
        print(f"  Kept No-Event: {len(kept_no_event):,} ({len(kept_no_event)/total*100:.1f}%)")
        print(f"  Total: {total:,}")
        print(f"\n  ğŸ“Š Final ratio:")
        print(f"     No-Event: {len(kept_no_event)/total*100:.1f}%")
        print(f"     Events: {num_events/total*100:.1f}%")

        return balanced
```

**Code Má»šI (ÄÃšNG) - OPTION 2 (Aggressive - Cho rare events):**
```python
class ClassBalancer:
    def __init__(self, config: Config):
        self.config = config

    def balance_dataset(self, windows: List[Dict], oversample_rare=True) -> List[Dict]:
        # TÃ¡ch theo class
        class_windows = defaultdict(list)
        for window in windows:
            class_windows[window['label']].append(window)

        # TÃ¬m sá»‘ lÆ°á»£ng cá»§a minority class (trá»« No-Event)
        event_counts = {label: len(samples) 
                       for label, samples in class_windows.items() 
                       if label != 'No-Event'}
        
        if not event_counts:
            return windows
        
        min_event_count = min(event_counts.values())
        max_event_count = max(event_counts.values())
        
        print(f"\nğŸ“Š Event class distribution:")
        for label, count in sorted(event_counts.items(), key=lambda x: -x[1]):
            print(f"  {label}: {count}")
        
        balanced = []
        
        # 1. Oversample rare events (Penalty, Red card)
        if oversample_rare:
            for label, samples in class_windows.items():
                if label == 'No-Event':
                    continue
                
                count = len(samples)
                
                # Náº¿u class quÃ¡ Ã­t (< 500), oversample lÃªn Ã­t nháº¥t 500
                if count < 500:
                    target = 500
                    # Duplicate samples
                    oversampled = samples * (target // count) + \
                                 random.sample(samples, target % count)
                    balanced.extend(oversampled)
                    print(f"  â¬†ï¸ Oversampled {label}: {count} â†’ {len(oversampled)}")
                else:
                    balanced.extend(samples)
        else:
            # KhÃ´ng oversample, chá»‰ giá»¯ nguyÃªn events
            for label, samples in class_windows.items():
                if label != 'No-Event':
                    balanced.extend(samples)
        
        # 2. Downsample No-Event
        num_events = len(balanced)
        no_event_samples = class_windows['No-Event']
        
        # No-Event = 2x events (hoáº·c dÃ¹ng config)
        target_no_event = num_events * 2
        num_to_keep = min(target_no_event, len(no_event_samples))
        kept_no_event = random.sample(no_event_samples, num_to_keep)
        
        balanced.extend(kept_no_event)
        random.shuffle(balanced)
        
        # Statistics
        total = len(balanced)
        print(f"\nğŸ¯ Final Balanced Dataset:")
        print(f"  Events: {num_events:,} ({num_events/total*100:.1f}%)")
        print(f"  No-Event: {len(kept_no_event):,} ({len(kept_no_event)/total*100:.1f}%)")
        print(f"  Total: {total:,}")
        
        return balanced
```

---

### **Fix 3: Training Arguments - ThÃªm Class Weights**

**ThÃªm TRÆ¯á»šC cell Train Model (Section 8):**

```python
# Compute class weights Ä‘á»ƒ penalize No-Event bias
from sklearn.utils.class_weight import compute_class_weight

print("\nğŸ’ª Computing class weights...")

# Get all label IDs
train_labels = [config.label2id[w['label']] for w in train_data]
unique_labels = np.unique(train_labels)

# Compute balanced weights
class_weights = compute_class_weight(
    'balanced',
    classes=unique_labels,
    y=train_labels
)

print("ğŸ“Š Class weights:")
for idx, weight in enumerate(class_weights):
    label = config.id2label[idx]
    print(f"  {label:20s}: {weight:.2f}")

# Convert to tensor
class_weights_tensor = torch.FloatTensor(class_weights)
```

**Sá»­a Trainer class:**

```python
from torch import nn

class WeightedTrainer(Trainer):
    """Custom Trainer with weighted loss"""
    
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        # Use weighted CrossEntropy
        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss

# Sá»­ dá»¥ng WeightedTrainer thay vÃ¬ Trainer
trainer = WeightedTrainer(  # â† Äá»•i tá»« Trainer
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)
```

---

### **Fix 4: Metric Optimization - Äá»•i tá»« f1_weighted sang f1_macro**

**Code CÅ¨:**
```python
training_args = TrainingArguments(
    # ...
    metric_for_best_model="f1_weighted",  # âŒ Bias vá» No-Event
    # ...
)
```

**Code Má»šI:**
```python
training_args = TrainingArguments(
    output_dir=config.output_dir,
    num_train_epochs=config.num_epochs,
    per_device_train_batch_size=config.batch_size,
    per_device_eval_batch_size=config.batch_size * 2,
    learning_rate=config.learning_rate,
    warmup_steps=config.warmup_steps,
    weight_decay=config.weight_decay,
    logging_dir=f"{config.output_dir}/logs",
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    
    # âœ… Äá»•i metric
    metric_for_best_model="f1_macro",  # Quan tÃ¢m Ä‘áº¿n táº¥t cáº£ classes
    
    greater_is_better=True,
    save_total_limit=3,
    report_to="none",
    fp16=torch.cuda.is_available(),
)
```

---

## ğŸš€ Tá»I Æ¯U CHO COLAB FREE

### **Váº¥n Ä‘á» GPU háº¿t nhanh:**

Google Colab Free cÃ³ giá»›i háº¡n:
- **GPU runtime: ~12 giá»/ngÃ y**
- **Sau khi háº¿t, pháº£i chá» ~12-24 giá» Ä‘á»ƒ reset**

### **Giáº£i phÃ¡p:**

#### **1. Giáº£m dataset size (Recommended)**

**ThÃªm vÃ o Config:**
```python
@dataclass
class Config:
    # ... (giá»¯ nguyÃªn)
    
    # âœ… THÃŠM: Giá»›i háº¡n sá»‘ matches Ä‘á»ƒ train nhanh hÆ¡n
    max_matches_to_use: int = 300  # Thay vÃ¬ 1420 matches
    
    # âœ… Giáº£m epochs cho láº§n Ä‘áº§u test
    num_epochs: int = 3  # Thay vÃ¬ 5
```

**Sá»­a trong load_all_transcripts():**
```python
def load_all_transcripts(self) -> List[Dict]:
    all_matches = []
    match_folders = self.get_all_match_folders()
    print(f"Found {len(match_folders)} match folders")
    
    # âœ… THÃŠM: Giá»›i háº¡n sá»‘ matches
    if hasattr(self.config, 'max_matches_to_use') and self.config.max_matches_to_use:
        match_folders = match_folders[:self.config.max_matches_to_use]
        print(f"âš ï¸ Limited to {len(match_folders)} matches for faster training")
    
    for match_folder in tqdm(match_folders, desc="Loading transcripts"):
        # ... (code cÅ©)
```

**Káº¿t quáº£:**
- 1420 matches â†’ 300 matches = giáº£m ~80% thá»i gian load
- Training time: 2h â†’ ~30-40 phÃºt

#### **2. Gradient Accumulation (Quan trá»ng!)**

**ThÃªm vÃ o TrainingArguments:**
```python
training_args = TrainingArguments(
    # ... (giá»¯ nguyÃªn)
    
    # âœ… THÃŠM gradient accumulation
    gradient_accumulation_steps=4,  # Effective batch = 16 * 4 = 64
    
    # Giáº£m batch size náº¿u bá»‹ OOM
    per_device_train_batch_size=8,  # Tá»« 16 xuá»‘ng 8
    per_device_eval_batch_size=16,
)
```

**Lá»£i Ã­ch:**
- Batch size thá»±c = 8 * 4 = 32 (vá»«a Ä‘á»§ lá»›n)
- Giáº£m VRAM usage
- KhÃ´ng áº£nh hÆ°á»Ÿng performance

#### **3. Mixed Precision Training (ÄÃ£ cÃ³)**

```python
fp16=torch.cuda.is_available(),  # âœ… ÄÃ£ cÃ³, giá»¯ nguyÃªn
```

#### **4. Checkpoint Resume (QUAN TRá»ŒNG!)**

**ThÃªm trÆ°á»›c trainer.train():**

```python
# Check for existing checkpoint
checkpoint_dir = None
if os.path.exists(config.output_dir):
    checkpoints = [d for d in os.listdir(config.output_dir) 
                   if d.startswith('checkpoint-')]
    if checkpoints:
        # Get latest checkpoint
        checkpoint_nums = [int(c.split('-')[1]) for c in checkpoints]
        latest = checkpoints[checkpoint_nums.index(max(checkpoint_nums))]
        checkpoint_dir = os.path.join(config.output_dir, latest)
        print(f"âœ… Found checkpoint: {checkpoint_dir}")

# Train with resume
print("ğŸš€ Starting training...\n")
print("="*70)

if checkpoint_dir:
    print(f"â–¶ï¸ Resuming from: {checkpoint_dir}\n")
    trainer.train(resume_from_checkpoint=checkpoint_dir)
else:
    trainer.train()

print("\nâœ… Training complete!")
```

**Lá»£i Ã­ch:**
- Náº¿u Colab disconnect, cÃ³ thá»ƒ tiáº¿p tá»¥c tá»« checkpoint
- KhÃ´ng máº¥t progress

#### **5. Clear GPU Memory**

**ThÃªm á»Ÿ Ä‘áº§u notebook (sau imports):**

```python
# Clear GPU cache
import gc
torch.cuda.empty_cache()
gc.collect()
print("ğŸ§¹ GPU cache cleared")
```

#### **6. Monitor GPU Usage**

**ThÃªm cell má»›i Ä‘á»ƒ track:**

```python
# Monitor GPU during training
!nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv --loop=1
```

---

## ğŸ“Š Káº¾T QUáº¢ MONG Äá»¢I SAU KHI FIX

### **Dataset Distribution:**
```
TRÆ¯á»šC FIX:
  No-Event: 158,833 (91.8%)  âŒ
  Events:    14,124 (8.2%)

SAU FIX:
  No-Event:  ~28,000 (40-50%)  âœ…
  Events:    ~14,000-18,000 (50-60%)  âœ…
```

### **Model Performance:**
```
TRÆ¯á»šC FIX:
  F1 Macro: 0.46  âŒ
  Goal F1:  0.43  âŒ
  
SAU FIX:
  F1 Macro: 0.65-0.75  âœ…
  Goal F1:  0.65-0.75  âœ…
  Penalty F1: 0.40-0.55  âœ…
```

### **Training Time:**
```
Full dataset (1420 matches):
  - 2 giá» (nhÆ° hiá»‡n táº¡i)
  
Limited (300 matches):
  - ~30-40 phÃºt âœ…
  
With checkpointing:
  - CÃ³ thá»ƒ pause/resume báº¥t cá»© lÃºc nÃ o âœ…
```

---

## ğŸ”§ CHECKLIST THá»°C HIá»†N

### **BÆ°á»›c 1: Backup**
- [ ] Download notebook hiá»‡n táº¡i
- [ ] Save model cÅ© (náº¿u cÃ³)

### **BÆ°á»›c 2: Sá»­a Config (Cell 7)**
```python
# Sá»­a dÃ²ng 355-357
no_event_multiplier: int = 2  # Thay vÃ¬ no_event_keep_ratio
```

### **BÆ°á»›c 3: Sá»­a ClassBalancer (Cell ~11)**
- [ ] Replace toÃ n bá»™ class `ClassBalancer`
- [ ] DÃ¹ng Option 1 hoáº·c Option 2 (recommend Option 2 náº¿u cÃ³ nhiá»u rare events)

### **BÆ°á»›c 4: ThÃªm Class Weights (Cell má»›i trÆ°á»›c training)**
- [ ] Add cell compute class weights
- [ ] Define `WeightedTrainer` class
- [ ] Äá»•i `Trainer` â†’ `WeightedTrainer`

### **BÆ°á»›c 5: Sá»­a TrainingArguments (Cell 8)**
- [ ] `metric_for_best_model="f1_macro"`
- [ ] `gradient_accumulation_steps=4`
- [ ] Giáº£m `per_device_train_batch_size=8` náº¿u OOM

### **BÆ°á»›c 6: Tá»‘i Æ°u (Optional nhÆ°ng strongly recommended)**
- [ ] Add `max_matches_to_use=300` vÃ o Config
- [ ] Add checkpoint resume logic
- [ ] Add GPU monitoring

### **BÆ°á»›c 7: Train**
- [ ] Run all cells
- [ ] Monitor GPU usage
- [ ] Check label distribution sau balance
- [ ] Verify No-Event ~40-50%

### **BÆ°á»›c 8: Evaluate**
- [ ] Check F1 Macro (target: >0.65)
- [ ] Check per-class F1
- [ ] Test trÃªn full match (nhÆ° Ä‘Ã£ test trÆ°á»›c)

---

## â° GPU FREE RESET TIME

**Khi nÃ o GPU reset?**
- Colab Free: ~12-24 giá» sau khi háº¿t quota
- Kiá»ƒm tra: Settings â†’ Resource limits

**Tips:**
1. Train trong giá» tháº¥p Ä‘iá»ƒm (2-5 AM EST)
2. DÃ¹ng 300 matches cho iteration Ä‘áº§u
3. Save checkpoint thÆ°á»ng xuyÃªn
4. Náº¿u gáº§n háº¿t GPU, save model ngay

---

## ğŸ“ FILE CODE HOÃ€N CHá»ˆNH

TÃ´i Ä‘Ã£ táº¡o file riÃªng vá»›i code Ä‘áº§y Ä‘á»§ Ä‘á»ƒ báº¡n copy-paste:
- `Config_Fixed.py` - Config má»›i
- `ClassBalancer_Fixed.py` - Balance logic má»›i
- `WeightedTrainer.py` - Trainer vá»›i class weights
- `Training_Cell_Complete.py` - Cell training Ä‘áº§y Ä‘á»§

---

## â“ FAQ

**Q: Fix nÃ y cÃ³ cháº¯c cháº¯n cáº£i thiá»‡n khÃ´ng?**
A: CÃ³ 95% cháº¯c cháº¯n. Váº¥n Ä‘á» chÃ­nh lÃ  imbalance, fix nÃ y giáº£i quyáº¿t Ä‘Ãºng root cause.

**Q: Máº¥t bao lÃ¢u Ä‘á»ƒ retrain?**
A: 
- Full (1420 matches): ~2h
- Limited (300 matches): ~30-40 phÃºt
- Recommend: Test vá»›i 300 matches trÆ°á»›c, náº¿u tá»‘t thÃ¬ cháº¡y full

**Q: Náº¿u váº«n khÃ´ng cáº£i thiá»‡n?**
A: Thá»­:
1. TÄƒng `no_event_multiplier` tá»« 2 â†’ 3
2. Oversample rare events (Option 2)
3. Thá»­ Focal Loss thay vÃ¬ Weighted CE
4. Augment data (paraphrase text)

**Q: Sau khi fix, threshold cÃ²n cáº§n Ä‘iá»u chá»‰nh khÃ´ng?**
A: CÃ³ thá»ƒ cáº§n tune láº¡i:
- Goal: 0.6-0.7 (tháº¥p hÆ¡n trÆ°á»›c)
- Substitution: 0.85-0.9 (giáº£m tá»« 0.95)

---

**Good luck! ğŸš€**

Náº¿u cÃ³ váº¥n Ä‘á», ping láº¡i vá»›i:
1. Label distribution sau balance
2. Training logs
3. Evaluation results
