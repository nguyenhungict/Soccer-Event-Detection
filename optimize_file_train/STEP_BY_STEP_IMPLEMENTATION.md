# STEP_BY_STEP_IMPLEMENTATION.md
# ğŸ”§ HÆ°á»›ng dáº«n tá»«ng bÆ°á»›c FIX vÃ  RETRAIN model

## ğŸ“‹ CHUáº¨N Bá»Š

### 1. Backup
```bash
# Download notebook hiá»‡n táº¡i
# Save model cÅ© (náº¿u Ä‘Ã£ train)
# Screenshot káº¿t quáº£ cÅ© Ä‘á»ƒ so sÃ¡nh
```

### 2. Hiá»ƒu váº¥n Ä‘á»
```
HIá»†N Táº I:
- No-Event: 158,833 (91.8%) â† QUÃ CAO
- Events: 14,124 (8.2%)
- F1 Macro: 0.46 â† KÃ‰M

Má»¤C TIÃŠU:
- No-Event: ~40-50%
- Events: ~50-60%
- F1 Macro: >0.65
```

---

## ğŸ”¨ BÆ¯á»šC 1: Sá»¬A CONFIG

**Location:** Cell 7 (dÃ²ng ~342-390)

### TÃ¬m dÃ²ng nÃ y:
```python
no_event_keep_ratio: float = 0.15
```

### Äá»•i thÃ nh:
```python
# âœ… FIXED: Downsample No-Event dá»±a trÃªn sá»‘ events
no_event_multiplier: int = 2  # No-Event = 2x events

# Optional: Giá»›i háº¡n matches Ä‘á»ƒ train nhanh
max_matches_to_use: int = 300  # Set None Ä‘á»ƒ dÃ¹ng háº¿t

# Giáº£m epochs cho iteration Ä‘áº§u
num_epochs: int = 3  # Tá»« 5 xuá»‘ng 3

# Giáº£m batch size
batch_size: int = 8  # Tá»« 16 xuá»‘ng 8

# ThÃªm gradient accumulation
gradient_accumulation_steps: int = 4
```

### Code Ä‘áº§y Ä‘á»§:
```python
@dataclass
class Config:
    dataset_root: str = "./dataset/sn-echoes/Dataset"
    whisper_folders: List[str] = None
    soccernet_labels_dir: str = "./dataset/soccernet"
    output_dir: str = "./models/soccer_event_temporal"
    
    reaction_lag_start: int = 1
    reaction_lag_end: int = 6
    context_window_size: int = 3
    
    # âœ… FIXED
    no_event_multiplier: int = 2
    max_matches_to_use: int = 300  # None for full dataset
    
    model_name: str = "xlm-roberta-base"
    max_length: int = 160
    
    # âœ… OPTIMIZED
    batch_size: int = 8
    learning_rate: float = 2e-5
    num_epochs: int = 3
    warmup_steps: int = 1000
    weight_decay: float = 0.01
    train_val_split: float = 0.8
    gradient_accumulation_steps: int = 4
    
    event_classes: List[str] = None
    
    def __post_init__(self):
        if self.whisper_folders is None:
            self.whisper_folders = ["whisper_v1_en", "whisper_v2_en"]
        if self.event_classes is None:
            self.event_classes = [
                "No-Event", "Goal", "Yellow card", "Red card", "Substitution", "Penalty"
            ]
        self.label2id = {label: idx for idx, label in enumerate(self.event_classes)}
        self.id2label = {idx: label for idx, label in enumerate(self.event_classes)}

config = Config()
print("âœ… Config loaded (Fixed Version)")
print(f"   Balance: No-Event = {config.no_event_multiplier}x Events")
if config.max_matches_to_use:
    print(f"   Limited: {config.max_matches_to_use} matches")
```

---

## ğŸ”¨ BÆ¯á»šC 2: Sá»¬A CLASS BALANCER

**Location:** Cell ~11 (ClassBalancer class)

### THAY THáº¾ TOÃ€N Bá»˜ CLASS:

```python
class ClassBalancer:
    def __init__(self, config):
        self.config = config
    
    def balance_dataset(self, windows, oversample_rare=True, min_rare_samples=500):
        # TÃ¡ch theo class
        class_windows = defaultdict(list)
        for window in windows:
            class_windows[window['label']].append(window)
        
        # Thá»‘ng kÃª events
        event_counts = {label: len(samples) 
                       for label, samples in class_windows.items() 
                       if label != 'No-Event'}
        
        if not event_counts:
            return windows
        
        print(f"\nğŸ“Š Original Event Distribution:")
        for label, count in sorted(event_counts.items(), key=lambda x: -x[1]):
            print(f"  {label:20s}: {count:6,d}")
        
        balanced = []
        oversampled_stats = {}
        
        # Step 1: Oversample rare events
        for label, samples in class_windows.items():
            if label == 'No-Event':
                continue
            
            count = len(samples)
            
            if oversample_rare and count < min_rare_samples:
                target = min_rare_samples
                num_copies = target // count
                remainder = target % count
                
                oversampled = samples * num_copies
                if remainder > 0:
                    oversampled.extend(random.sample(samples, remainder))
                
                balanced.extend(oversampled)
                oversampled_stats[label] = (count, len(oversampled))
            else:
                balanced.extend(samples)
        
        # Step 2: Downsample No-Event
        num_events = len(balanced)
        no_event_samples = class_windows['No-Event']
        
        multiplier = getattr(self.config, 'no_event_multiplier', 2)
        target_no_event = num_events * multiplier
        num_to_keep = min(target_no_event, len(no_event_samples))
        
        kept_no_event = random.sample(no_event_samples, num_to_keep)
        balanced.extend(kept_no_event)
        random.shuffle(balanced)
        
        # Statistics
        total = len(balanced)
        print(f"\nğŸ¯ BALANCING RESULTS:")
        print("="*60)
        
        if oversampled_stats:
            print("ğŸ“ˆ Oversampled:")
            for label, (before, after) in oversampled_stats.items():
                print(f"  {label:20s}: {before:6,d} â†’ {after:6,d}")
            print()
        
        print(f"ğŸ“Š Final:")
        print(f"  Events:   {num_events:,d} ({num_events/total*100:.1f}%)")
        print(f"  No-Event: {len(kept_no_event):,d} ({len(kept_no_event)/total*100:.1f}%)")
        print(f"  Total:    {total:,d}")
        
        final_counts = defaultdict(int)
        for w in balanced:
            final_counts[w['label']] += 1
        
        print(f"\nğŸ“‹ Per-class:")
        for label, count in sorted(final_counts.items(), key=lambda x: -x[1]):
            pct = count / total * 100
            print(f"  {label:20s}: {count:7,d} ({pct:5.1f}%)")
        print("="*60)
        
        no_event_ratio = len(kept_no_event) / total
        if 0.3 <= no_event_ratio <= 0.6:
            print("âœ… Balance looks good!")
        else:
            print("âš ï¸ WARNING: Check no_event_multiplier")
        
        return balanced

print("âœ… ClassBalancer (Fixed v2) defined")
```

---

## ğŸ”¨ BÆ¯á»šC 3: THÃŠM CLASS WEIGHTS

**Location:** Cell Má»šI, thÃªm SAU cell load model, TRÆ¯á»šC cell training

### ThÃªm cell Má»šI:

```python
# ============================================================================
# COMPUTE CLASS WEIGHTS
# ============================================================================

import torch.nn as nn
from sklearn.utils.class_weight import compute_class_weight

print("\nğŸ’ª Computing class weights...")

# Get labels from training data
train_labels = [config.label2id[w['label']] for w in train_data]
unique_labels = np.unique(train_labels)

# Compute balanced weights
class_weights = compute_class_weight(
    'balanced',
    classes=unique_labels,
    y=train_labels
)

print("\nğŸ“Š Class Weights:")
print("-" * 50)
for idx, weight in enumerate(class_weights):
    label = config.id2label[idx]
    count = train_labels.count(idx)
    print(f"  {label:20s}: {weight:6.2f} (n={count:,})")
print("-" * 50)

# Convert to tensor
class_weights_tensor = torch.FloatTensor(class_weights)

# ============================================================================
# WEIGHTED TRAINER
# ============================================================================

class WeightedLossTrainer(Trainer):
    def __init__(self, class_weights=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights
        print("âœ… WeightedLossTrainer initialized")
    
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        if self.class_weights is not None:
            loss_fct = nn.CrossEntropyLoss(
                weight=self.class_weights.to(logits.device)
            )
        else:
            loss_fct = nn.CrossEntropyLoss()
        
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), 
                       labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss

print("âœ… Class weights computed")
```

---

## ğŸ”¨ BÆ¯á»šC 4: Sá»¬A TRAINING ARGUMENTS

**Location:** Cell 8 (Training cell)

### TÃ¬m vÃ  sá»­a:

```python
training_args = TrainingArguments(
    output_dir=config.output_dir,
    num_train_epochs=config.num_epochs,
    per_device_train_batch_size=config.batch_size,
    per_device_eval_batch_size=config.batch_size * 2,
    learning_rate=config.learning_rate,
    warmup_steps=config.warmup_steps,
    weight_decay=config.weight_decay,
    
    logging_dir=f"{config.output_dir}/logs",
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=3,
    
    load_best_model_at_end=True,
    
    # âœ… CHANGED: f1_weighted â†’ f1_macro
    metric_for_best_model="f1_macro",
    greater_is_better=True,
    
    # âœ… ADDED: Gradient accumulation
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    
    fp16=torch.cuda.is_available(),
    report_to="none",
)
```

### Äá»•i Trainer:

```python
# âœ… CHANGED: Trainer â†’ WeightedLossTrainer
trainer = WeightedLossTrainer(
    class_weights=class_weights_tensor,  # âœ… Add class weights
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

print("âœ… Trainer ready with weighted loss")
```

---

## ğŸ”¨ BÆ¯á»šC 5: THÃŠM CHECKPOINT RESUME (Optional nhÆ°ng recommended)

**Location:** Trong cell training, TRÆ¯á»šC trainer.train()

### ThÃªm code nÃ y:

```python
# ============================================================================
# CHECKPOINT RESUME
# ============================================================================

def find_latest_checkpoint(output_dir):
    if not os.path.exists(output_dir):
        return None
    
    checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    if not checkpoints:
        return None
    
    checkpoint_steps = []
    for cp in checkpoints:
        try:
            step = int(os.path.basename(cp).split('-')[1])
            checkpoint_steps.append((step, cp))
        except:
            continue
    
    if checkpoint_steps:
        checkpoint_steps.sort(reverse=True)
        return checkpoint_steps[0][1]
    return None

# Check for checkpoint
checkpoint_path = find_latest_checkpoint(config.output_dir)

if checkpoint_path:
    print(f"âœ… Found checkpoint: {checkpoint_path}")
    print("   Will resume training...")
else:
    print("â„¹ï¸ No checkpoint found. Starting fresh.")

# ============================================================================
# TRAIN
# ============================================================================

print("\nğŸš€ Starting training...")
print("="*70)

if checkpoint_path:
    trainer.train(resume_from_checkpoint=checkpoint_path)
else:
    trainer.train()

print("\nâœ… Training complete!")
```

---

## ğŸ”¨ BÆ¯á»šC 6: OPTIONAL - GIá»šI Háº N Sá» MATCHES

**Location:** Cell load transcripts (Section 7)

### Sá»­a trong TranscriptLoader:

```python
def load_all_transcripts(self) -> List[Dict]:
    all_matches = []
    match_folders = self.get_all_match_folders()
    print(f"Found {len(match_folders)} match folders")
    
    # âœ… ADDED: Limit matches
    if hasattr(self.config, 'max_matches_to_use') and self.config.max_matches_to_use:
        match_folders = match_folders[:self.config.max_matches_to_use]
        print(f"âš ï¸ Limited to {len(match_folders)} matches for faster training")
    
    for match_folder in tqdm(match_folders, desc="Loading transcripts"):
        # ... rest of code
```

---

## âœ… VERIFICATION CHECKLIST

Sau khi sá»­a, CHECK cÃ¡c Ä‘iá»ƒm sau TRÆ¯á»šC KHI TRAIN:

### [ ] Config
- [ ] `no_event_multiplier = 2` (khÃ´ng pháº£i no_event_keep_ratio)
- [ ] `batch_size = 8`
- [ ] `num_epochs = 3`
- [ ] `gradient_accumulation_steps = 4`
- [ ] `max_matches_to_use = 300` (hoáº·c None cho full)

### [ ] ClassBalancer
- [ ] Balance logic dÃ¹ng multiplier, khÃ´ng pháº£i ratio
- [ ] CÃ³ oversample cho rare events
- [ ] Print ra statistics Ä‘áº§y Ä‘á»§

### [ ] Training Setup
- [ ] Class weights Ä‘Æ°á»£c compute
- [ ] WeightedLossTrainer Ä‘Æ°á»£c dÃ¹ng
- [ ] `metric_for_best_model = "f1_macro"`
- [ ] CÃ³ checkpoint resume logic

### [ ] Dataset Loading
- [ ] CÃ³ giá»›i háº¡n matches (náº¿u muá»‘n train nhanh)
- [ ] Load thÃ nh cÃ´ng transcripts vÃ  labels

---

## ğŸš€ CHáº Y TRAINING

### 1. Run tá»«ng cell tá»« Ä‘áº§u
```python
# Cell 1: GPU check âœ“
# Cell 2: Install dependencies âœ“
# Cell 3: Mount Drive âœ“
# Cell 4: Download labels (skip náº¿u Ä‘Ã£ cÃ³) âœ“
# Cell 5: Check dataset âœ“
# Cell 6: Imports âœ“
# Cell 7: Config (FIXED) âœ“
# Cells 8-12: Helper classes (FIXED ClassBalancer) âœ“
# Cell 13: Load transcripts âœ“
# Cell 14: Temporal alignment âœ“
# Cell 15: Balance (CHECK OUTPUT!) âœ“
# Cell 16: Split data âœ“
# Cell 17: Load model âœ“
# Cell NEW: Compute class weights âœ“
# Cell 18: Training (FIXED) âœ“
```

### 2. Monitor sau Cell 15 (Balance)

**PHáº¢I THáº¤Y:**
```
ğŸ“Š Final:
  Events:   ~14,000-18,000 (50-60%)  â† QUAN TRá»ŒNG!
  No-Event: ~14,000-28,000 (40-50%)  â† QUAN TRá»ŒNG!
  Total:    ~28,000-36,000

âœ… Balance looks good!
```

**Náº¾U THáº¤Y:**
```
No-Event: 91.8%  â† VáºªN SAI!
```
â†’ STOP, kiá»ƒm tra láº¡i ClassBalancer

### 3. Monitor sau Cell Class Weights

**PHáº¢I THáº¤Y:**
```
ğŸ“Š Class Weights:
  No-Event:  0.50-1.00
  Goal:      2.00-4.00  â† Higher than No-Event
  Penalty:   10.00-30.00  â† Highest
```

### 4. Monitor training logs

**Good signs:**
```
Epoch 1/3: Loss giáº£m dáº§n
Eval: F1 Macro tÄƒng dáº§n (0.50 â†’ 0.60 â†’ 0.70)
GPU usage: 70-90%
```

**Bad signs:**
```
Loss khÃ´ng giáº£m hoáº·c tÄƒng â†’ Check learning rate
GPU usage: <50% â†’ Check batch size
OOM error â†’ Giáº£m batch_size xuá»‘ng 4
```

---

## ğŸ“Š Káº¾T QUáº¢ MONG Äá»¢I

### Sau Balance:
```
TRÆ¯á»šC:
  No-Event: 158,833 (91.8%)  âŒ

SAU:
  No-Event: ~28,000 (40-50%)  âœ…
  Events: ~28,000 (50-60%)  âœ…
```

### Sau Training:
```
TRÆ¯á»šC:
  F1 Macro: 0.46
  Goal F1: 0.43
  Penalty F1: 0.21

SAU (Target):
  F1 Macro: 0.65-0.75  âœ…
  Goal F1: 0.65-0.75  âœ…
  Penalty F1: 0.40-0.55  âœ…
```

### Training Time:
```
300 matches: 30-40 phÃºt
1420 matches: ~2 giá»
```

---

## âš ï¸ TROUBLESHOOTING

### Váº¥n Ä‘á» 1: No-Event váº«n 90%+
**NguyÃªn nhÃ¢n:** ClassBalancer khÃ´ng Ä‘Æ°á»£c sá»­a Ä‘Ãºng
**Fix:** Copy láº¡i code ClassBalancer tá»« file `ClassBalancer_Fixed_v2.py`

### Váº¥n Ä‘á» 2: OOM (Out of Memory)
**NguyÃªn nhÃ¢n:** Batch size quÃ¡ lá»›n
**Fix:** 
```python
batch_size: int = 4  # Giáº£m xuá»‘ng 4
gradient_accumulation_steps: int = 8  # TÄƒng lÃªn 8
```

### Váº¥n Ä‘á» 3: Training quÃ¡ cháº­m
**Fix:**
```python
max_matches_to_use: int = 100  # Giáº£m xuá»‘ng 100
num_epochs: int = 2
```

### Váº¥n Ä‘á» 4: F1 Macro khÃ´ng cáº£i thiá»‡n
**Thá»­:**
1. TÄƒng `no_event_multiplier` tá»« 2 â†’ 1
2. TÄƒng `min_rare_samples` tá»« 500 â†’ 800
3. Thá»­ Focal Loss thay vÃ¬ Weighted CE

### Váº¥n Ä‘á» 5: Colab disconnect
**Fix:** 
- ÄÃ£ cÃ³ checkpoint resume â†’ Cháº¡y láº¡i, nÃ³ sáº½ tá»± resume
- Náº¿u khÃ´ng cÃ³ checkpoint â†’ Pháº£i train láº¡i tá»« Ä‘áº§u

---

## ğŸ“ SUPPORT

Náº¿u gáº·p váº¥n Ä‘á», cáº§n check:
1. **Label distribution sau balance** - Paste output Cell 15
2. **Class weights** - Paste output cell compute weights
3. **Training logs** - Screenshot epoch 1-2
4. **Evaluation results** - Classification report

Good luck! ğŸš€
